# Redu√ß√£o de Dimensionalidade em Machine Learning

Redu√ß√£o de dimensionalidade √© o processo de **diminuir o n√∫mero de vari√°veis (features)** de um conjunto de dados, mantendo ao m√°ximo as informa√ß√µes relevantes. √â uma etapa crucial no **pr√©-processamento de dados** para melhorar desempenho, reduzir custo computacional e evitar overfitting.

---

## üìö Por que reduzir a dimensionalidade?

- **Eliminar redund√¢ncia e ru√≠do** nos dados  
- **Melhorar performance de algoritmos** (especialmente em dados de alta dimens√£o)
- **Visualizar dados complexos** em 2D ou 3D
- **Evitar a maldi√ß√£o da dimensionalidade**  
  (quando muitos atributos pioram a performance dos modelos)

---

## üß© Tipos de Redu√ß√£o de Dimensionalidade

| T√©cnica                          | Tipo       | Preserva vari√¢ncia? | Exemplo de uso                                   |
|----------------------------------|------------|----------------------|--------------------------------------------------|
| PCA (Principal Component Analysis) | Linear     | Sim                  | Imagens, dados tabulares                         |
| t-SNE (t-distributed Stochastic Neighbor Embedding) | N√£o linear | N√£o (foca em estrutura local) | Visualiza√ß√£o de dados em 2D/3D                   |
| UMAP (Uniform Manifold Approximation and Projection) | N√£o linear | Parcialmente           | Clustering e visualiza√ß√£o                        |
| Autoencoders (Redes Neurais)    | N√£o linear | Depende do treinamento| Compress√£o de dados, imagens, voz                |
| Sele√ß√£o de Features (e.g., SelectKBest) | Filtragem | Depende              | Modelos interpret√°veis, feature engineering      |

---

## üìñ Teoria: PCA (Principal Component Analysis)

O **PCA** √© uma t√©cnica estat√≠stica que transforma os dados originais em um novo espa√ßo de coordenadas, chamado de **componentes principais**, que s√£o:

1. **Ortogonais** entre si (sem correla√ß√£o)
2. Ordenados pela **vari√¢ncia** que explicam nos dados

### Etapas do PCA:
1. Centralizar os dados (remover a m√©dia)
2. Calcular a matriz de covari√¢ncia
3. Obter os autovalores e autovetores
4. Ordenar por maior vari√¢ncia
5. Projetar os dados nos novos eixos (componentes)

---

## üõ†Ô∏è Pr√°tica: Exemplo com PCA em Python

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Carregando os dados
data = load_iris()
X = data.data
y = data.target

# Redu√ß√£o para 2 dimens√µes
pca = PCA(n_components=2)
X_reduzido = pca.fit_transform(X)

# Visualiza√ß√£o
plt.scatter(X_reduzido[:, 0], X_reduzido[:, 1], c=y, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA - Iris Dataset')
plt.show()
```

## ‚öôÔ∏è Pr√°tica: Quando usar Redu√ß√£o de Dimensionalidade?

| Situa√ß√£o                                    | Usar? | T√©cnica recomendada         |
|---------------------------------------------|-------|------------------------------|
| Dados com muitas colunas correlacionadas    | ‚úÖ     | PCA ou Autoencoder           |
| Visualiza√ß√£o em 2D/3D                       | ‚úÖ     | t-SNE ou UMAP                |
| Explicar vari√°veis de forma interpret√°vel   | ‚ùå     | (Redu√ß√£o pode prejudicar)    |
| Dados de imagem, √°udio ou s√©ries temporais  | ‚úÖ     | Autoencoders ou PCA          |

---

## üöß Limita√ß√µes e Cuidados

- ‚ùó **Perda de interpretabilidade**: novos componentes podem ser dif√≠ceis de entender
- ‚ùó **Risco de perda de informa√ß√£o √∫til** se o n√∫mero de componentes for muito reduzido
- ‚ùó **Overfitting em autoencoders** se n√£o forem treinados corretamente
- ‚ùó t-SNE e UMAP n√£o servem para generaliza√ß√£o (s√≥ para visualiza√ß√£o)

---

## üìå Conclus√£o

Redu√ß√£o de dimensionalidade √© uma ferramenta poderosa para:
- Simplificar dados
- Acelerar o treinamento de modelos
- Visualizar padr√µes escondidos

Ela deve ser usada com **consci√™ncia do objetivo** (visualiza√ß√£o, desempenho, interpretabilidade) e sempre analisando o **trade-off entre simplifica√ß√£o e perda de informa√ß√£o**.
